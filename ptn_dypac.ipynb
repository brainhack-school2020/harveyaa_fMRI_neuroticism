{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dypac code to work with preprocessed time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dypac_timeseries\n",
    "from dypac_timeseries import Dypac,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Chopped up to use with preprocessed time series by annabelle harvey\n",
    "\"\"\"Dynamic Parcel Aggregation with Clustering (dypac).\"\"\"\n",
    "\n",
    "# Authors: Pierre Bellec, Amal Boukhdir\n",
    "# License: BSD 3 clause\n",
    "import warnings\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from joblib import Memory\n",
    "from nilearn.decomposition.base import BaseDecomposition\n",
    "\n",
    "class Dypac(BaseDecomposition):\n",
    "    \"\"\"\n",
    "    Perform Stable Dynamic Cluster Analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters: int, optional\n",
    "        Number of clusters to extract per time window\n",
    "\n",
    "    n_states: int, optional\n",
    "        Number of expected dynamic states\n",
    "\n",
    "    n_replications: int, optional\n",
    "        Number of replications of cluster analysis in each fMRI run\n",
    "\n",
    "    n_batch: int, optional\n",
    "        Number of batches to run through consensus clustering.\n",
    "        If n_batch<=1, consensus clustering will be applied\n",
    "        to all replications in one pass. Processing with batch will\n",
    "        reduce dramatically the compute time, but will change slightly\n",
    "        the results.\n",
    "\n",
    "    n_init: int, optional\n",
    "        Number of initializations for k-means\n",
    "\n",
    "    subsample_size: int, optional\n",
    "        Number of time points in a subsample\n",
    "\n",
    "    max_iter: int, optional\n",
    "        Max number of iterations for k-means\n",
    "\n",
    "    threshold_sim: float (0 <= . <= 1), optional\n",
    "        Minimal acceptable average dice in a state\n",
    "\n",
    "    random_state: int or RandomState, optional\n",
    "        Pseudo number generator state used for random sampling.\n",
    "\n",
    "    standardize: boolean, optional\n",
    "        If standardize is True, the time-series are centered and normed:\n",
    "        their mean is put to 0 and their variance to 1 in the time dimension.\n",
    "\n",
    "    detrend: boolean, optional\n",
    "        This parameter is passed to signal.clean. Please see the related\n",
    "        documentation for details\n",
    "\n",
    "    low_pass: None or float, optional\n",
    "        This parameter is passed to signal.clean. Please see the related\n",
    "        documentation for details\n",
    "\n",
    "    high_pass: None or float, optional\n",
    "        This parameter is passed to signal.clean. Please see the related\n",
    "        documentation for details\n",
    "\n",
    "    t_r: float, optional\n",
    "        This parameter is passed to signal.clean. Please see the related\n",
    "        documentation for details\n",
    "\n",
    "    memory: instance of joblib.Memory or str\n",
    "        Used to cache the masking process.\n",
    "        By default, no caching is done. If a string is given, it is the\n",
    "        path to the caching directory.\n",
    "\n",
    "    memory_level: integer, optional\n",
    "        Rough estimator of the amount of memory used by caching. Higher value\n",
    "        means more memory for caching.\n",
    "\n",
    "    verbose: integer, optional\n",
    "        Indicate the level of verbosity. By default, print progress.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_clusters=10,\n",
    "        n_states=3,\n",
    "        n_replications=40,\n",
    "        n_batch=1,\n",
    "        n_init=30,\n",
    "        n_init_aggregation=100,\n",
    "        subsample_size=30,\n",
    "        max_iter=30,\n",
    "        threshold_sim=0.3,\n",
    "        random_state=None,\n",
    "        standardize=True,\n",
    "        detrend=True,\n",
    "        low_pass=None,\n",
    "        high_pass=None,\n",
    "        t_r=None,\n",
    "        memory=Memory(cachedir=None),\n",
    "        memory_level=0,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        \"\"\"Set up default attributes for the class.\"\"\"\n",
    "        # All those settings are taken from nilearn BaseDecomposition\n",
    "        self.random_state = random_state\n",
    "        self.standardize = standardize\n",
    "        self.detrend = detrend\n",
    "        self.low_pass = low_pass\n",
    "        self.high_pass = high_pass\n",
    "        self.t_r = t_r\n",
    "        self.memory = memory\n",
    "        self.memory_level = max(0, memory_level + 1)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Those settings are specific to parcel aggregation\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_states = n_states\n",
    "        self.n_batch = n_batch\n",
    "        self.n_replications = n_replications\n",
    "        self.n_init = n_init\n",
    "        self.n_init_aggregation = n_init_aggregation\n",
    "        self.subsample_size = subsample_size\n",
    "        self.max_iter = max_iter\n",
    "        self.threshold_sim = threshold_sim\n",
    "\n",
    "    def _check_components_(self):\n",
    "        \"\"\"Check for presence of estimated components.\"\"\"\n",
    "        if not hasattr(self, \"components_\"):\n",
    "            raise ValueError(\n",
    "                \"Object has no components_ attribute. \"\n",
    "                \"This is probably because fit has not \"\n",
    "                \"been called.\"\n",
    "            )\n",
    "\n",
    "    def fit(self, series):\n",
    "        \"\"\"\n",
    "        Compute the mask and the dynamic parcels across datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        series: list of 4800x200 timeseries (timepoints x nodes)\n",
    " \n",
    "         Returns\n",
    "         -------\n",
    "         self: object\n",
    "            Returns the instance itself. Contains attributes listed\n",
    "            at the object level.\n",
    "        \"\"\"\n",
    "\n",
    "        # Control random number generation\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        # Check that number of batches is reasonable\n",
    "        if self.n_batch > len(series):\n",
    "            warnings.warn(\n",
    "                \"{0} batches were requested, but only {1} datasets available. Using {2} batches instead.\".format(\n",
    "                    self.n_batch, len(series), self.n_batch\n",
    "                )\n",
    "            )\n",
    "            self.n_batch = len(series)\n",
    "\n",
    "        # reduce step\n",
    "        if self.n_batch > 1:\n",
    "            stab_maps, dwell_time = self._reduce_batch(series)\n",
    "        else:\n",
    "            stab_maps, dwell_time = self._reduce(series)\n",
    "\n",
    "        # Return components\n",
    "        self.components_ = stab_maps\n",
    "        self.dwell_time_ = dwell_time\n",
    "\n",
    "        # Create embedding\n",
    "        self.embedding = Embedding(stab_maps.todense())\n",
    "        return self\n",
    "\n",
    "    def _reduce_batch(self, all_series):\n",
    "        \"\"\"Iterate dypac on batches of files.\"\"\"\n",
    "        stab_maps_list = []\n",
    "        dwell_time_list = []\n",
    "        for bb in range(self.n_batch):\n",
    "            slice_batch = slice(bb, len(all_series), self.n_batch)\n",
    "            if self.verbose:\n",
    "                print(\"[{0}] Processing batch {1}\".format(self.__class__.__name__, bb))\n",
    "            stab_maps, dwell_time = self._reduce(\n",
    "                all_series[slice_batch]\n",
    "            )\n",
    "            stab_maps_list.append(stab_maps)\n",
    "            dwell_time_list.append(dwell_time)\n",
    "\n",
    "        stab_maps_cons, dwell_time_cons = consensus_batch(\n",
    "            stab_maps_list,\n",
    "            dwell_time_list,\n",
    "            self.n_replications,\n",
    "            self.n_states,\n",
    "            self.max_iter,\n",
    "            self.n_init_aggregation,\n",
    "            self.random_state,\n",
    "            self.verbose,\n",
    "        )\n",
    "\n",
    "        return stab_maps_cons, dwell_time_cons\n",
    "\n",
    "    def _reduce(self, all_series):\n",
    "        \"\"\"\n",
    "        Cluster aggregation on a list of 4800x200 timeseries (timepoints x nodes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stab_maps: ndarray\n",
    "            stability maps of each state.\n",
    "\n",
    "        dwell_time: ndarray\n",
    "            dwell time of each state.\n",
    "        \"\"\"\n",
    "        onehot_list = []\n",
    "        for ind, time_series in zip(range(len(all_series)), all_series):\n",
    "            \n",
    "            onehot = replicate_clusters(\n",
    "                time_series.transpose(),\n",
    "                subsample_size=self.subsample_size,\n",
    "                n_clusters=self.n_clusters,\n",
    "                n_replications=self.n_replications,\n",
    "                max_iter=self.max_iter,\n",
    "                n_init=self.n_init,\n",
    "                random_state=self.random_state,\n",
    "                desc=\"Replicating clusters in data #{0}\".format(ind),\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            onehot_list.append(onehot)\n",
    "        onehot_all = vstack(onehot_list)\n",
    "        del onehot_list\n",
    "        del onehot\n",
    "\n",
    "        # find the states\n",
    "        states = find_states(\n",
    "            onehot_all,\n",
    "            n_states=self.n_states,\n",
    "            max_iter=self.max_iter,\n",
    "            threshold_sim=self.threshold_sim,\n",
    "            random_state=self.random_state,\n",
    "            n_init=self.n_init_aggregation,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "        # Generate the stability maps\n",
    "        stab_maps, dwell_time = produce_stab_maps(\n",
    "            onehot_all, states, self.n_replications, self.n_states\n",
    "        )\n",
    "\n",
    "        return stab_maps, dwell_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bagging analysis of stable clusters (BASC)++.\n",
    "Scalable and fast ensemble clustering.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Pierre Bellec, Amal Boukhdir\n",
    "# License: BSD 3 clause\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.sparse import csr_matrix, find, vstack\n",
    "\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def _select_subsample(y, subsample_size, start=None):\n",
    "    \"\"\"Select a random subsample in a data array.\"\"\"\n",
    "    n_samples = y.shape[1]\n",
    "    subsample_size = np.min([subsample_size, n_samples])\n",
    "    max_start = n_samples - subsample_size\n",
    "    if start is not None:\n",
    "        start = np.min([start, max_start])\n",
    "    else:\n",
    "        start = np.floor((max_start + 1) * np.random.rand(1))\n",
    "    stop = start + subsample_size\n",
    "    samp = y[:, np.arange(int(start), int(stop))]\n",
    "    return samp\n",
    "\n",
    "\n",
    "def _part2onehot(part, n_clusters=0):\n",
    "    \"\"\"\n",
    "    Convert a series of partition (one per row) with integer clusters into\n",
    "    a series of one-hot encoding vectors (one per row and cluster).\n",
    "    \"\"\"\n",
    "    if n_clusters == 0:\n",
    "        n_clusters = np.max(part) + 1\n",
    "    n_part, n_voxel = part.shape\n",
    "    n_el = n_part * n_voxel\n",
    "    val = np.repeat(True, n_el)\n",
    "    ind_r = np.reshape(part, n_el) + np.repeat(\n",
    "        np.array(range(n_part)) * n_clusters, n_voxel\n",
    "    )\n",
    "    ind_c = np.repeat(\n",
    "        np.reshape(range(n_voxel), [1, n_voxel]), n_part, axis=0\n",
    "    ).flatten()\n",
    "    s_onehot = [n_part * n_clusters, n_voxel]\n",
    "    onehot = csr_matrix((val, (ind_r, ind_c)), shape=s_onehot, dtype=\"bool\")\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def _start_window(n_time, n_replications, subsample_size):\n",
    "    \"\"\"Get a list of the starting points of sliding windows.\"\"\"\n",
    "    max_replications = n_time - subsample_size + 1\n",
    "    n_replications = np.min([max_replications, n_replications])\n",
    "    list_start = np.linspace(0, max_replications, n_replications)\n",
    "    list_start = np.floor(list_start)\n",
    "    list_start = np.unique(list_start)\n",
    "    return list_start\n",
    "\n",
    "\n",
    "def _trim_states(onehot, states, n_states, verbose, threshold_sim):\n",
    "    \"\"\"Trim the states clusters to exclude outliers.\"\"\"\n",
    "    for ss in tqdm(range(n_states), disable=not verbose, desc=\"Trimming states\"):\n",
    "        ix, iy, _ = find(onehot[states == ss, :])\n",
    "        size_onehot = np.array(onehot[states == ss, :].sum(axis=1)).flatten()\n",
    "        ref_cluster = np.array(onehot[states == ss, :].mean(dtype=\"float\", axis=0))\n",
    "        avg_stab = np.divide(\n",
    "            np.bincount(ix, weights=ref_cluster[0, iy].flatten()), size_onehot\n",
    "        )\n",
    "        tmp = states[states == ss]\n",
    "        tmp[avg_stab < threshold_sim] = n_states\n",
    "        states[states == ss] = tmp\n",
    "    return states\n",
    "\n",
    "\n",
    "def replicate_clusters(\n",
    "    y,\n",
    "    subsample_size,\n",
    "    n_clusters,\n",
    "    n_replications,\n",
    "    max_iter=100,\n",
    "    n_init=10,\n",
    "    random_state=None,\n",
    "    verbose=False,\n",
    "    embedding=np.array([]),\n",
    "    desc=\"\",\n",
    "    normalize=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Replicate a clustering on random subsamples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy array\n",
    "        size number of samples x number of features\n",
    "\n",
    "    subsample_size: int\n",
    "        The size of the subsample used to generate cluster replications\n",
    "\n",
    "    n_clusters: int\n",
    "        The number of clusters to be extracted by k-means.\n",
    "\n",
    "    n_replications: int\n",
    "        The number of replications\n",
    "\n",
    "    n_init: int, optional\n",
    "            Number of initializations for k-means\n",
    "\n",
    "    max_iter: int, optional\n",
    "        Max number of iterations for the k-means algorithm\n",
    "\n",
    "    verbose: boolean, optional\n",
    "        Turn on/off verbose\n",
    "\n",
    "    embedding: array, optional\n",
    "        if present, the embedding array will be appended to samp for each sample.\n",
    "        For example, embedding can be a set of spatial coordinates,\n",
    "        to encourage spatial proximity in the clusters.\n",
    "\n",
    "    desc: string, optional\n",
    "        message to insert in verbose\n",
    "\n",
    "    normalize: boolean, optional\n",
    "        turn on/off scaling of each sample to zero mean and unit variance\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    onehot: boolean, sparse array\n",
    "        onehot representation of clusters, stacked over all replications.\n",
    "    \"\"\"\n",
    "    list_start = _start_window(y.shape[1], n_replications, subsample_size)\n",
    "    if list_start.shape[0] < n_replications:\n",
    "        warnings.warn(\n",
    "            \"{0} replications were requested, but only {1} available.\".format(\n",
    "                n_replications, list_start.shape[0]\n",
    "            )\n",
    "        )\n",
    "    range_replication = range(list_start.shape[0])\n",
    "    part = np.zeros([list_start.shape[0], y.shape[0]], dtype=\"int\")\n",
    "\n",
    "    for rr in tqdm(range_replication, disable=not verbose, desc=desc):\n",
    "        if normalize:\n",
    "            samp = scale(_select_subsample(y, subsample_size, list_start[rr]), axis=1)\n",
    "        else:\n",
    "            samp = _select_subsample(y, subsample_size, list_start[rr])\n",
    "        if embedding.shape[0] > 0:\n",
    "            samp = np.concatenate(\n",
    "                [_select_subsample(y, subsample_size, list_start[rr]), embedding],\n",
    "                axis=1,\n",
    "            )\n",
    "        _, part[rr, :], _ = k_means(\n",
    "            samp,\n",
    "            n_clusters=n_clusters,\n",
    "            init=\"k-means++\",\n",
    "            max_iter=max_iter,\n",
    "            n_init=n_init,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    return _part2onehot(part, n_clusters)\n",
    "\n",
    "\n",
    "def find_states(\n",
    "    onehot,\n",
    "    n_states=10,\n",
    "    max_iter=30,\n",
    "    threshold_sim=0.3,\n",
    "    n_init=10,\n",
    "    random_state=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Find dynamic states based on the similarity of clusters over time.\"\"\"\n",
    "    if verbose:\n",
    "        print(\"Consensus clustering.\")\n",
    "    _, states, _ = k_means(\n",
    "        onehot,\n",
    "        n_clusters=n_states,\n",
    "        init=\"k-means++\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        n_init=n_init,\n",
    "    )\n",
    "    states = _trim_states(onehot, states, n_states, verbose, threshold_sim)\n",
    "    return states\n",
    "\n",
    "\n",
    "def produce_stab_maps(onehot, states, n_replications, n_states, dwell_time_all=None):\n",
    "    \"\"\"Generate stability maps associated with different states.\"\"\"\n",
    "    # Dwell times\n",
    "    dwell_time = np.zeros(n_states)\n",
    "    for ss in range(0, n_states):\n",
    "        if np.any(dwell_time_all == None):\n",
    "            dwell_time[ss] = np.sum(states == ss) / n_replications\n",
    "        else:\n",
    "            dwell_time[ss] = np.mean(dwell_time_all[states == ss])\n",
    "    # Re-order stab maps by descending dwell time\n",
    "    indsort = np.argsort(-dwell_time)\n",
    "    dwell_time = dwell_time[indsort]\n",
    "\n",
    "    # Stability maps\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    val = []\n",
    "    for idx, ss in enumerate(indsort):\n",
    "        if np.any(states == ss):\n",
    "            stab_map = onehot[states == ss, :].mean(dtype=\"float\", axis=0)\n",
    "            mask = stab_map > 0\n",
    "\n",
    "            row_ind.append(np.repeat(idx, np.sum(mask)))\n",
    "            col_ind.append(np.nonzero(mask)[1])\n",
    "            val.append(np.array(stab_map[mask]).flatten())\n",
    "    stab_maps = csr_matrix(\n",
    "        (np.concatenate(val), (np.concatenate(row_ind), np.concatenate(col_ind))),\n",
    "        shape=[n_states, onehot.shape[1]],\n",
    "    )\n",
    "\n",
    "    return stab_maps, dwell_time\n",
    "\n",
    "\n",
    "def consensus_batch(\n",
    "    stab_maps_list,\n",
    "    dwell_time_list,\n",
    "    n_replications,\n",
    "    n_states=10,\n",
    "    max_iter=30,\n",
    "    n_init=10,\n",
    "    random_state=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    stab_maps_all = vstack(stab_maps_list)\n",
    "    del stab_maps_list\n",
    "    dwell_time_all = np.concatenate(dwell_time_list)\n",
    "    del dwell_time_list\n",
    "\n",
    "    # Consensus clustering step\n",
    "    if verbose:\n",
    "        print(\"Inter-batch consensus\")\n",
    "    _, states_all, _ = k_means(\n",
    "        stab_maps_all,\n",
    "        n_clusters=n_states,\n",
    "        sample_weight=dwell_time_all,\n",
    "        init=\"k-means++\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        n_init=n_init,\n",
    "    )\n",
    "\n",
    "    # average stability maps and dwell times across consensus states\n",
    "    if verbose:\n",
    "        print(\"Generating consensus stability maps\")\n",
    "    stab_maps_cons, dwell_time_cons = produce_stab_maps(\n",
    "        stab_maps_all, states_all, n_replications, n_states, dwell_time_all\n",
    "    )\n",
    "    return stab_maps_cons, dwell_time_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def miss_constant(X, precision=1e-10):\n",
    "    \"\"\"Check if a constant vector is missing in a vector basis.\n",
    "    \"\"\"\n",
    "    return np.min(np.sum(np.absolute(X - 1), axis=1)) > precision\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, X, add_constant=True):\n",
    "        \"\"\"\n",
    "        Transformation to and from an embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray\n",
    "            The vector basis defining the embedding (each row is a vector).\n",
    "\n",
    "        add_constant: boolean\n",
    "            Add a constant vector to the vector basis, if none is present.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        size: int\n",
    "            the number of vectors defining the embedding, not including the\n",
    "            intercept.\n",
    "\n",
    "        transform_mat: ndarray\n",
    "            matrix projection from original to embedding space.\n",
    "\n",
    "        inverse_transform_mat: ndarray\n",
    "            matrix projection from embedding to original space.\n",
    "\n",
    "        \"\"\"\n",
    "        self.size = X.shape[0]\n",
    "        # Once we have the embedded representation beta, the inverse transform\n",
    "        # is a simple linear mixture:\n",
    "        # Y_hat = beta * X\n",
    "        # We store X as the inverse transform matrix\n",
    "        if add_constant and miss_constant(X):\n",
    "            self.inverse_transform_mat = np.concatenate([np.ones([1, X.shape[1]]), X])\n",
    "        else:\n",
    "            self.inverse_transform_mat = X\n",
    "        # The embedded representation beta is also derived by a simple linear\n",
    "        # mixture Y * P, where P is the pseudo-inverse of X\n",
    "        # We store P as our transform matrix\n",
    "        self.transform_mat = np.linalg.pinv(self.inverse_transform_mat)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Project data in embedding space.\"\"\"\n",
    "        # Given Y, we get\n",
    "        # beta = Y * P\n",
    "        return np.matmul(data, self.transform_mat)\n",
    "\n",
    "    def inverse_transform(self, embedded_data):\n",
    "        \"\"\"Project embedded data back to original space.\"\"\"\n",
    "        # Given beta, we get:\n",
    "        # Y_hat = beta * X\n",
    "        return np.matmul(embedded_data, self.inverse_transform_mat)\n",
    "\n",
    "    def compress(self, data):\n",
    "        \"\"\"Embedding compression of data in original space.\"\"\"\n",
    "        # Given Y, by combining transform and inverse_transform, we get:\n",
    "        # Y_hat = Y * P * X\n",
    "        return self.inverse_transform(self.transform(data))\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"Average residual squares after compress in embedding space.\"\"\"\n",
    "        # The R2 score is only interpretable for standardized data\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "        return 1 - np.var(data - self.compress(data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './HCP_PTN1200_recon2/NodeTimeseries_3T_HCP1200_MSMAll_ICAd200_ts2/node_timeseries/3T_HCP1200_MSMAll_d200_ts2/{}.txt'\n",
    "series = []\n",
    "#DONT PICK NULL SUBJECTS: [109830, 614439]\n",
    "subjects = [100206,100610]\n",
    "for sub in subjects:\n",
    "    series.append(np.loadtxt(path.format(sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replicating clusters in data #0: 100%|██████████| 40/40 [00:11<00:00,  3.51it/s]\n",
      "Replicating clusters in data #1: 100%|██████████| 40/40 [00:08<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus clustering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trimming states: 100%|██████████| 3/3 [00:00<00:00, 172.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dypac(detrend=True, high_pass=None, low_pass=None, max_iter=30,\n",
       "      memory=Memory(location=None), memory_level=1, n_batch=1, n_clusters=10,\n",
       "      n_init=30, n_init_aggregation=100, n_replications=40, n_states=3,\n",
       "      random_state=RandomState(MT19937) at 0x7F0A92243050, standardize=True,\n",
       "      subsample_size=30, t_r=None, threshold_sim=0.3, verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dypac = Dypac()\n",
    "dypac.fit(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from nilearn import image, plotting\n",
    "path2 = './HCP_PTN1200_recon2/groupICA_3T_HCP1200_MSMAll/groupICA/groupICA_3T_HCP1200_MSMAll_d200.ica/{}'\n",
    "ICA_map = image.load_img(path2.format('melodic_IC_sum.nii'))\n",
    "masker = NiftiMapsMasker(ICA_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker.fit()\n",
    "num_comp = 1 # the component number\n",
    "comp = masker.inverse_transform(dypac.components_[num_comp,:].todense())\n",
    "plotting.view_img(comp, threshold=0.1, vmax=1, title=\"Dwell time: {dt}\".format(dt=dypac.dwell_time_[num_comp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
